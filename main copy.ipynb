{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-09 07:52:14.915153: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-09 07:52:14.939943: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-09 07:52:14.939977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-09 07:52:14.940668: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-09 07:52:14.945387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 07:52:15.611027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob as glob\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    " \n",
    " \n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from urllib.request import urlretrieve\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    " \n",
    "seed_everything(42)\n",
    " \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch dataset where label is the image name.jpg and the image is the image itself\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, processor,  transform=None , max_target_length=128):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imgs = list(sorted(os.listdir(root)))\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root, self.imgs[index])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        label = self.imgs[index][:-4]\n",
    "        pixel_values = self.processor(img, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        # i.e. tokenized labels.\n",
    "        labels = self.processor.tokenizer(\n",
    "            label,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "data_path = \"archive\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "from torchvision import transforms\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\" , use_fast=False)\n",
    "\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4),\n",
    "    \n",
    "]) \n",
    "\n",
    "#keep 0.1 data aside for testing\n",
    "dataset = ImageDataset(data_path, processor, transform=transforms)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size] , generator=torch.Generator().manual_seed(42))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"/home/arnesh/Desktop/CAPTCHA MODEL/models/0.08231476569407588_17_20240107T042532\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44187/2117612379.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda memory clear\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:31:19<00:00,  1.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1696/1696 [10:17<00:00,  2.75it/s]\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.15545892854895083\n",
      "Train CER:  0.03588222287735849\n",
      "Val CER:  0.05316966838193254\n",
      "-----------------------------------------------------------\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:30:39<00:00,  1.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1696/1696 [10:19<00:00,  2.74it/s]\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.16642942145397616\n",
      "Train CER:  0.03993956367924529\n",
      "Val CER:  0.04931300028587765\n",
      "-----------------------------------------------------------\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:30:37<00:00,  1.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1696/1696 [10:27<00:00,  2.70it/s]\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.16804780812519723\n",
      "Train CER:  0.03976513364779874\n",
      "Val CER:  0.05456778873642082\n",
      "-----------------------------------------------------------\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:31:04<00:00,  1.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1696/1696 [10:34<00:00,  2.67it/s]\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.16371020441133538\n",
      "Train CER:  0.039028105345911945\n",
      "Val CER:  0.04410913379073757\n",
      "-----------------------------------------------------------\n",
      "Epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:30:40<00:00,  1.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1696/1696 [10:18<00:00,  2.74it/s]\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.1549769294729488\n",
      "Train CER:  0.034879864386792456\n",
      "Val CER:  0.056731525157232704\n",
      "-----------------------------------------------------------\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:30:39<00:00,  1.25it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1696/1696 [10:21<00:00,  2.73it/s]\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/arnesh/ai/yolo/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:  0.1567444037246221\n",
      "Train CER:  0.035317167845911955\n",
      "Val CER:  0.04798009576901086\n",
      "-----------------------------------------------------------\n",
      "Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6784/6784 [1:30:33<00:00,  1.25it/s]\n",
      " 15%|â–ˆâ–Œ        | 256/1696 [01:34<08:54,  2.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(val_dataloader):\n\u001b[0;32m---> 66\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixel_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     cer \u001b[38;5;241m=\u001b[39m compute_cer(pred_ids\u001b[38;5;241m=\u001b[39moutputs, label_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     68\u001b[0m     val_cer\u001b[38;5;241m.\u001b[39mappend(cer)\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/utils.py:1516\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;66;03m# two conditions must be met\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;66;03m# 1) the generation config must have been created from the model config (`_from_model_config` field);\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# 2) the generation config must have seen no modification since its creation (the hash is the same).\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_original_object_hash \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mhash\u001b[39m(\n\u001b[1;32m   1514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m   1515\u001b[0m     ):\n\u001b[0;32m-> 1516\u001b[0m         new_generation_config \u001b[38;5;241m=\u001b[39m \u001b[43mGenerationConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_generation_config \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config:\n\u001b[1;32m   1518\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1519\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have modified the pretrained model configuration to control generation. This is a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m deprecated strategy to control generation and will be removed soon, in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1521\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use and modify the model generation configuration (see\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1523\u001b[0m             )\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:942\u001b[0m, in \u001b[0;36mGenerationConfig.from_model_config\u001b[0;34m(cls, model_config)\u001b[0m\n\u001b[1;32m    940\u001b[0m config_dict \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m    941\u001b[0m config_dict\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_model_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 942\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_model_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Special case: some models have generation attributes set in the decoder. Use them if still unset in the\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# generation config.\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:826\u001b[0m, in \u001b[0;36mGenerationConfig.from_dict\u001b[0;34m(cls, config_dict, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs})\n\u001b[1;32m    824\u001b[0m unused_kwargs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 826\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate config \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_unused_kwargs:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config, unused_kwargs\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:358\u001b[0m, in \u001b[0;36mGenerationConfig.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:903\u001b[0m, in \u001b[0;36mGenerationConfig.to_json_string\u001b[0;34m(self, use_diff, ignore_metadata)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03mSerializes this instance to a JSON string.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m    `str`: String containing all the attributes that make up this configuration instance in JSON format.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_diff \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 903\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_diff_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:852\u001b[0m, in \u001b[0;36mGenerationConfig.to_diff_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_diff_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;124;03m    Removes all attributes from config which correspond to the default config attributes for better readability and\u001b[39;00m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    serializes to a Python dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m        `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 852\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# get the default config dict\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     default_config_dict \u001b[38;5;241m=\u001b[39m GenerationConfig()\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/ai/yolo/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:874\u001b[0m, in \u001b[0;36mGenerationConfig.to_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    868\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m    Serializes this instance to a Python dictionary.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \n\u001b[1;32m    871\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;124;03m        `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;66;03m# Fields to ignore at serialization time\u001b[39;00m\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output:\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.11/copy.py:128\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    124\u001b[0m     d[PyStringMap] \u001b[38;5;241m=\u001b[39m PyStringMap\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m d, t\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeepcopy\u001b[39m(x, memo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _nil\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#0.1 percent test data and k fold cross validation with 5 folds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(train_dataset)\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_cer = []\n",
    "val_cer = []\n",
    "\n",
    "#train using pytorch library without using trainer\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_cer = []\n",
    "val_cer = []\n",
    "val_cer1 = []\n",
    "train_cer1 = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    for train_index, val_index in kf.split(train_dataset):\n",
    "        # Shuffle indices manually\n",
    "        np.random.shuffle(train_index)\n",
    "        np.random.shuffle(val_index)\n",
    "\n",
    "        train_dataset_split = Subset(train_dataset, train_index)\n",
    "        val_dataset_split = Subset(train_dataset, val_index)\n",
    "        train_dataloader = DataLoader(train_dataset_split, batch_size=12, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset_split, batch_size=12, shuffle=True)\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "        model.train()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "            for k,v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(loss.item())\n",
    "            labels = batch['labels'].to(device)\n",
    "            input_ids = batch['pixel_values'].to(device)\n",
    "            pred_ids = model.generate(input_ids)\n",
    "            cer = compute_cer(pred_ids, labels)\n",
    "            train_cer.append(cer)\n",
    "\n",
    "        model.eval()\n",
    "        for batch in tqdm(val_dataloader):\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            val_cer.append(cer)\n",
    "\n",
    "        print(\"Train Loss: \", np.mean(train_loss))\n",
    "        print(\"Train CER: \", np.mean(train_cer))\n",
    "        print(\"Val CER: \", np.mean(val_cer))\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "\n",
    "        val_cer1.append(np.mean(val_cer))\n",
    "        train_cer1.append(np.mean(train_cer))\n",
    "\n",
    "        \n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        train_cer = []\n",
    "        val_cer = []\n",
    "\n",
    "        # Save every epoch with cer score\n",
    "        save_pretrained_dir = f'models_val/{np.mean(val_cer)}_{epoch}_'\n",
    "        model.save_pretrained(save_pretrained_dir)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pretrained model for inference\n",
    "save_pretrained_dir = 'final_model'\n",
    "model.save_pretrained(save_pretrained_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1aXSz']\n"
     ]
    }
   ],
   "source": [
    "#inference setup\n",
    "from transformers import TrOCRProcessor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"final_model\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "#load example image\n",
    "\n",
    "img_path = \"archive/1aXSz.jpg\"\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "pixel_values = processor(img, return_tensors='pt').pixel_values\n",
    "outputs = model.generate(pixel_values.to(device))\n",
    "pred_str = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(pred_str) #print prediction\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
